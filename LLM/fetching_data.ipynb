{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata만 있는 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가격 정보 누락: 맥윙™\n",
      "가격 정보 누락: 맥윙™콤보\n",
      "가격 정보 누락: 골든 모짜렐라 치즈스틱\n",
      "가격 정보 누락: 맥너겟®\n",
      "가격 정보 누락: 맥스파이시®치킨 텐더\n",
      "가격 정보 누락: 해쉬 브라운\n",
      "가격 정보 누락: 딸기 오레오 맥플러리\n",
      "가격 정보 누락: 초코 오레오 맥플러리\n",
      "가격 정보 누락: 스트로베리콘\n",
      "가격 정보 누락: 아이스크림콘\n",
      "가격 정보 누락: 바닐라 선데이 아이스크림\n",
      "가격 정보 누락: 초코 선데이 아이스크림\n",
      "가격 정보 누락: 딸기 선데이 아이스크림\n",
      "가격 정보 누락: 오레오 아포가토\n",
      "가격 정보 누락: 아이스 카페라떼\n",
      "가격 정보 누락: 디카페인 아이스 카페라떼\n",
      "가격 정보 누락: 아이스 아메리카노\n",
      "가격 정보 누락: 디카페인 아이스 아메리카노\n",
      "가격 정보 누락: 아이스 드립 커피\n",
      "가격 정보 누락: 바닐라 쉐이크 Medium\n",
      "가격 정보 누락: 딸기 쉐이크 Medium\n",
      "가격 정보 누락: 초코 쉐이크 Medium\n",
      "가격 정보 누락: 오렌지 주스\n",
      "메뉴 데이터를 파일에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime  # datetime 모듈 추가\n",
    "\n",
    "class McDonaldsMenu:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.mcdonalds.co.kr/kor/menu/listContent.do\"\n",
    "        self.detail_url = \"https://www.mcdonalds.co.kr/kor/menu/detail.do\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        self.menu_prices = {}\n",
    "\n",
    "    def fetch_menu_data(self, sub_category_seq, page_num):\n",
    "        data = {\n",
    "            \"page\": page_num,\n",
    "            \"sub_category_seq\": sub_category_seq\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('list', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_menu_detail(self, menu_code, sub_category_seq):\n",
    "        data = {\"seq\": menu_code, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            menu_name_tag = soup.find('h2', class_='ko')\n",
    "            menu_desc_tag = soup.find('div', class_='desc')\n",
    "\n",
    "            menu_name = menu_name_tag.get_text(strip=True) if menu_name_tag else \"메뉴 이름을 찾을 수 없음\"\n",
    "            cleaned_menu_name = re.sub(r'\\s+', '', menu_name)\n",
    "            cleaned_menu_name = self.clean_text(cleaned_menu_name)\n",
    "\n",
    "            menu_desc = menu_desc_tag.get_text(strip=True) if menu_desc_tag else \"메뉴 설명을 찾을 수 없음\"\n",
    "            menu_desc = re.sub(r'[™®]', '', menu_desc)\n",
    "            menu_desc = re.sub(r'\\*?판매\\s*시간\\s*[:：]?\\s*[^\\*]+', '', menu_desc).strip()\n",
    "\n",
    "            menu_price = self.menu_prices.get(cleaned_menu_name, \"가격 정보를 찾을 수 없음\")\n",
    "\n",
    "            if menu_price == \"가격 정보를 찾을 수 없음\":\n",
    "                print(f\"가격 정보 누락: {menu_name}\")\n",
    "\n",
    "            return {\"menu_name\": menu_name, \"menu_desc\": menu_desc, \"menu_price\": menu_price}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"상세 정보 API 요청 실패: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fetch_price_data(self):\n",
    "        for cat_id in range(11, 16):\n",
    "            if cat_id == 12:\n",
    "                continue\n",
    "            url = f\"https://www.mcdelivery.co.kr/kr/browse/menu.html?daypartId=1&catId={cat_id}\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            script_content = soup.find_all('script', string=re.compile('ecommerce'))\n",
    "\n",
    "            if not script_content:\n",
    "                print(f\"catId {cat_id}: 데이터를 포함한 스크립트를 찾지 못했습니다.\")\n",
    "            else:\n",
    "                name_price_pattern = re.compile(r\"'name'\\s*:\\s*\\\"(.*?)\\\".*?'price'\\s*:\\s*'(\\d+)\", re.DOTALL)\n",
    "\n",
    "                for script in script_content:\n",
    "                    matches = name_price_pattern.findall(script.string)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            name, price = match\n",
    "                            clean_name = re.sub(r'\\s+', '', name)\n",
    "                            clean_name = self.clean_text(clean_name)\n",
    "                            self.menu_prices[clean_name] = price\n",
    "\n",
    "    def fetch_nutrition_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', class_='tableType01 nutrDesc')\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                nutrition_info = [[col.text.strip() for col in row.find_all(['th', 'td'])] for row in rows]\n",
    "                return nutrition_info\n",
    "            else:\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"영양 정보 API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_origin_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            origin_info_section = soup.find('ul', class_='origin_info')\n",
    "            if origin_info_section:\n",
    "                origin_info = origin_info_section.get_text(strip=True)\n",
    "                # 괄호와 그 안의 내용을 제거합니다\n",
    "                origin_info = re.sub(r'\\([^)]*\\)', '', origin_info)\n",
    "                # 불필요한 공백을 제거합니다\n",
    "                origin_info = re.sub(r'\\s+', ' ', origin_info).strip()\n",
    "                return re.sub(r'[™®]', '', origin_info)\n",
    "            else:\n",
    "                return \"원산지 정보 없음\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return \"원산지 정보 요청 실패\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        cleaned_text = re.sub(r'[™®]', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_nutrition_info(self, nutrition_data):\n",
    "        if not nutrition_data:\n",
    "            return \"영양소 정보 없음\"\n",
    "        headers = nutrition_data[0]\n",
    "        values = nutrition_data[1]\n",
    "        formatted_nutrition = {header: value for header, value in zip(headers, values) if header != \"영양소\"}\n",
    "        # 모든 값을 문자열로 변환\n",
    "        return {k: str(v) for k, v in formatted_nutrition.items()}\n",
    "\n",
    "    def save_menus_to_file_and_return_json(self, all_menu_items_by_category):\n",
    "        category_names = {\n",
    "            1: \"버거\",\n",
    "            7: \"사이드\",\n",
    "            8: \"디저트\",\n",
    "            9: \"맥카페\",\n",
    "            10: \"음료\",\n",
    "        }\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        today = datetime.now().strftime(\"%m%d\")\n",
    "        file_path = f\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_{today}_meta.json\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sub_category_seq, all_menu_items in all_menu_items_by_category.items():\n",
    "                category_name = category_names.get(sub_category_seq, \"기타\")\n",
    "                for item in all_menu_items:\n",
    "                    kor_name = self.clean_text(item.get('kor_name', ''))\n",
    "                    menu_code = item.get('seq', '')\n",
    "                    menu_details = self.fetch_menu_detail(menu_code, sub_category_seq)\n",
    "\n",
    "                    nutrition_data = self.format_nutrition_info(self.fetch_nutrition_data(menu_code, sub_category_seq))\n",
    "                    origin_info = self.fetch_origin_data(menu_code, sub_category_seq)\n",
    "\n",
    "                    document = {\n",
    "                        \"metadata\": {\n",
    "                            \"category\": category_name,\n",
    "                            \"name\": kor_name,\n",
    "                            \"description\": menu_details[\"menu_desc\"],\n",
    "                            \"price\": str(menu_details[\"menu_price\"]),  # 문자열로 변환\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin_info\": origin_info\n",
    "                        }\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "                    \n",
    "            json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"메뉴 데이터를 파일에 저장했습니다.\")\n",
    "        return json.dumps(documents, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "menu = McDonaldsMenu()\n",
    "menu.fetch_price_data()\n",
    "\n",
    "all_menu_items_by_category = {}\n",
    "for sub_category_seq in [1, 7, 8, 9, 10]:\n",
    "    all_menu_items = []\n",
    "    for page_num in range(1, 6):\n",
    "        menu_data = menu.fetch_menu_data(sub_category_seq, page_num)\n",
    "        if not menu_data:\n",
    "            break\n",
    "        all_menu_items.extend(menu_data)\n",
    "    all_menu_items_by_category[sub_category_seq] = all_menu_items\n",
    "\n",
    "json_data = menu.save_menus_to_file_and_return_json(all_menu_items_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagecontent + Metadata 둘 다 빽빽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가격 정보 누락: 맥윙™\n",
      "가격 정보 누락: 맥윙™콤보\n",
      "가격 정보 누락: 골든 모짜렐라 치즈스틱\n",
      "가격 정보 누락: 맥너겟®\n",
      "가격 정보 누락: 맥스파이시®치킨 텐더\n",
      "가격 정보 누락: 해쉬 브라운\n",
      "가격 정보 누락: 딸기 오레오 맥플러리\n",
      "가격 정보 누락: 초코 오레오 맥플러리\n",
      "가격 정보 누락: 스트로베리콘\n",
      "가격 정보 누락: 아이스크림콘\n",
      "가격 정보 누락: 바닐라 선데이 아이스크림\n",
      "가격 정보 누락: 초코 선데이 아이스크림\n",
      "가격 정보 누락: 딸기 선데이 아이스크림\n",
      "가격 정보 누락: 오레오 아포가토\n",
      "가격 정보 누락: 아이스 카페라떼\n",
      "가격 정보 누락: 디카페인 아이스 카페라떼\n",
      "가격 정보 누락: 아이스 아메리카노\n",
      "가격 정보 누락: 디카페인 아이스 아메리카노\n",
      "가격 정보 누락: 아이스 드립 커피\n",
      "가격 정보 누락: 바닐라 쉐이크 Medium\n",
      "가격 정보 누락: 딸기 쉐이크 Medium\n",
      "가격 정보 누락: 초코 쉐이크 Medium\n",
      "가격 정보 누락: 오렌지 주스\n",
      "메뉴 데이터를 파일에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime  # datetime 모듈 추가\n",
    "\n",
    "class McDonaldsMenu:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.mcdonalds.co.kr/kor/menu/listContent.do\"\n",
    "        self.detail_url = \"https://www.mcdonalds.co.kr/kor/menu/detail.do\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        self.menu_prices = {}\n",
    "\n",
    "    def fetch_menu_data(self, sub_category_seq, page_num):\n",
    "        data = {\n",
    "            \"page\": page_num,\n",
    "            \"sub_category_seq\": sub_category_seq\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('list', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_menu_detail(self, menu_code, sub_category_seq):\n",
    "        data = {\"seq\": menu_code, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            menu_name_tag = soup.find('h2', class_='ko')\n",
    "            menu_desc_tag = soup.find('div', class_='desc')\n",
    "\n",
    "            menu_name = menu_name_tag.get_text(strip=True) if menu_name_tag else \"메뉴 이름을 찾을 수 없음\"\n",
    "            cleaned_menu_name = re.sub(r'\\s+', '', menu_name)\n",
    "            cleaned_menu_name = self.clean_text(cleaned_menu_name)\n",
    "\n",
    "            menu_desc = menu_desc_tag.get_text(strip=True) if menu_desc_tag else \"메뉴 설명을 찾을 수 없음\"\n",
    "            menu_desc = re.sub(r'[™®]', '', menu_desc)\n",
    "            menu_desc = re.sub(r'\\*?판매\\s*시간\\s*[:：]?\\s*[^\\*]+', '', menu_desc).strip()\n",
    "\n",
    "            menu_price = self.menu_prices.get(cleaned_menu_name, \"가격 정보를 찾을 수 없음\")\n",
    "\n",
    "            if menu_price == \"가격 정보를 찾을 수 없음\":\n",
    "                print(f\"가격 정보 누락: {menu_name}\")\n",
    "\n",
    "            return {\"menu_name\": menu_name, \"menu_desc\": menu_desc, \"menu_price\": menu_price}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"상세 정보 API 요청 실패: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fetch_price_data(self):\n",
    "        for cat_id in range(11, 16):\n",
    "            if cat_id == 12:\n",
    "                continue\n",
    "            url = f\"https://www.mcdelivery.co.kr/kr/browse/menu.html?daypartId=1&catId={cat_id}\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            script_content = soup.find_all('script', string=re.compile('ecommerce'))\n",
    "\n",
    "            if not script_content:\n",
    "                print(f\"catId {cat_id}: 데이터를 포함한 스크립트를 찾지 못했습니다.\")\n",
    "            else:\n",
    "                name_price_pattern = re.compile(r\"'name'\\s*:\\s*\\\"(.*?)\\\".*?'price'\\s*:\\s*'(\\d+)\", re.DOTALL)\n",
    "\n",
    "                for script in script_content:\n",
    "                    matches = name_price_pattern.findall(script.string)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            name, price = match\n",
    "                            clean_name = re.sub(r'\\s+', '', name)\n",
    "                            clean_name = self.clean_text(clean_name)\n",
    "                            self.menu_prices[clean_name] = price\n",
    "\n",
    "    def fetch_nutrition_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', class_='tableType01 nutrDesc')\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                nutrition_info = [[col.text.strip() for col in row.find_all(['th', 'td'])] for row in rows]\n",
    "                return nutrition_info\n",
    "            else:\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"영양 정보 API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_origin_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            origin_info_section = soup.find('ul', class_='origin_info')\n",
    "            if origin_info_section:\n",
    "                origin_info = origin_info_section.get_text(strip=True)\n",
    "                # 괄호와 그 안의 내용을 제거합니다\n",
    "                origin_info = re.sub(r'\\([^)]*\\)', '', origin_info)\n",
    "                # 불필요한 공백을 제거합니다\n",
    "                origin_info = re.sub(r'\\s+', ' ', origin_info).strip()\n",
    "                return re.sub(r'[™®]', '', origin_info)\n",
    "            else:\n",
    "                return \"원산지 정보 없음\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return \"원산지 정보 요청 실패\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        cleaned_text = re.sub(r'[™®]', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_nutrition_info(self, nutrition_data):\n",
    "        if not nutrition_data:\n",
    "            return \"영양소 정보 없음\"\n",
    "        headers = nutrition_data[0]\n",
    "        values = nutrition_data[1]\n",
    "        formatted_nutrition = {header: value for header, value in zip(headers, values) if header != \"영양소\"}\n",
    "        # 모든 값을 문자열로 변환\n",
    "        return {k: str(v) for k, v in formatted_nutrition.items()}\n",
    "\n",
    "    def save_menus_to_file_and_return_json(self, all_menu_items_by_category):\n",
    "        category_names = {\n",
    "            1: \"버거\",\n",
    "            7: \"사이드\",\n",
    "            8: \"디저트\",\n",
    "            9: \"맥카페\",\n",
    "            10: \"음료\",\n",
    "        }\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        today = datetime.now().strftime(\"%m%d\")\n",
    "        file_path = f\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_{today}_all.json\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sub_category_seq, all_menu_items in all_menu_items_by_category.items():\n",
    "                category_name = category_names.get(sub_category_seq, \"기타\")\n",
    "                for item in all_menu_items:\n",
    "                    kor_name = self.clean_text(item.get('kor_name', ''))\n",
    "                    menu_code = item.get('seq', '')\n",
    "                    menu_details = self.fetch_menu_detail(menu_code, sub_category_seq)\n",
    "\n",
    "                    nutrition_data = self.format_nutrition_info(self.fetch_nutrition_data(menu_code, sub_category_seq))\n",
    "                    origin_info = self.fetch_origin_data(menu_code, sub_category_seq)\n",
    "\n",
    "                    document = {\n",
    "                        \"page_content\": {\n",
    "                            \"name\": kor_name,\n",
    "                            \"description\": menu_details[\"menu_desc\"],\n",
    "                            \"price\": str(menu_details[\"menu_price\"]),  # 문자열로 변환\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin\": origin_info\n",
    "                        },\n",
    "                        \"metadata\": {\n",
    "                            \"category\": category_name,\n",
    "                            \"price\": str(menu_details[\"menu_price\"]),  # 문자열로 변환\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin_info\": origin_info\n",
    "                        }\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "\n",
    "            json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"메뉴 데이터를 파일에 저장했습니다.\")\n",
    "        return json.dumps(documents, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "menu = McDonaldsMenu()\n",
    "menu.fetch_price_data()\n",
    "\n",
    "all_menu_items_by_category = {}\n",
    "for sub_category_seq in [1, 7, 8, 9, 10]:\n",
    "    all_menu_items = []\n",
    "    for page_num in range(1, 6):\n",
    "        menu_data = menu.fetch_menu_data(sub_category_seq, page_num)\n",
    "        if not menu_data:\n",
    "            break\n",
    "        all_menu_items.extend(menu_data)\n",
    "    all_menu_items_by_category[sub_category_seq] = all_menu_items\n",
    "\n",
    "json_data = menu.save_menus_to_file_and_return_json(all_menu_items_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagecontent 간단 ver + Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가격 정보 누락: 맥윙™\n",
      "가격 정보 누락: 맥윙™콤보\n",
      "가격 정보 누락: 골든 모짜렐라 치즈스틱\n",
      "가격 정보 누락: 맥너겟®\n",
      "가격 정보 누락: 맥스파이시®치킨 텐더\n",
      "가격 정보 누락: 해쉬 브라운\n",
      "가격 정보 누락: 딸기 오레오 맥플러리\n",
      "가격 정보 누락: 초코 오레오 맥플러리\n",
      "가격 정보 누락: 스트로베리콘\n",
      "가격 정보 누락: 아이스크림콘\n",
      "가격 정보 누락: 바닐라 선데이 아이스크림\n",
      "가격 정보 누락: 초코 선데이 아이스크림\n",
      "가격 정보 누락: 딸기 선데이 아이스크림\n",
      "가격 정보 누락: 오레오 아포가토\n",
      "가격 정보 누락: 아이스 카페라떼\n",
      "가격 정보 누락: 디카페인 아이스 카페라떼\n",
      "가격 정보 누락: 아이스 아메리카노\n",
      "가격 정보 누락: 디카페인 아이스 아메리카노\n",
      "가격 정보 누락: 아이스 드립 커피\n",
      "가격 정보 누락: 바닐라 쉐이크 Medium\n",
      "가격 정보 누락: 딸기 쉐이크 Medium\n",
      "가격 정보 누락: 초코 쉐이크 Medium\n",
      "가격 정보 누락: 오렌지 주스\n",
      "메뉴 데이터를 파일에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "class McDonaldsMenu:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.mcdonalds.co.kr/kor/menu/listContent.do\"\n",
    "        self.detail_url = \"https://www.mcdonalds.co.kr/kor/menu/detail.do\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        self.menu_prices = {}\n",
    "\n",
    "    def fetch_menu_data(self, sub_category_seq, page_num):\n",
    "        data = {\n",
    "            \"page\": page_num,\n",
    "            \"sub_category_seq\": sub_category_seq\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('list', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_menu_detail(self, menu_code, sub_category_seq):\n",
    "        data = {\"seq\": menu_code, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            menu_name_tag = soup.find('h2', class_='ko')\n",
    "            menu_desc_tag = soup.find('div', class_='desc')\n",
    "\n",
    "            menu_name = menu_name_tag.get_text(strip=True) if menu_name_tag else \"메뉴 이름을 찾을 수 없음\"\n",
    "            cleaned_menu_name = re.sub(r'\\s+', '', menu_name)\n",
    "            cleaned_menu_name = self.clean_text(cleaned_menu_name)\n",
    "\n",
    "            menu_desc = menu_desc_tag.get_text(strip=True) if menu_desc_tag else \"메뉴 설명을 찾을 수 없음\"\n",
    "            menu_desc = re.sub(r'[™®]', '', menu_desc)\n",
    "            menu_desc = re.sub(r'\\*?판매\\s*시간\\s*[:：]?\\s*[^\\*]+', '', menu_desc).strip()\n",
    "\n",
    "            menu_price = self.menu_prices.get(cleaned_menu_name, \"가격 정보를 찾을 수 없음\")\n",
    "\n",
    "            if menu_price == \"가격 정보를 찾을 수 없음\":\n",
    "                print(f\"가격 정보 누락: {menu_name}\")\n",
    "\n",
    "            return {\"menu_name\": menu_name, \"menu_desc\": menu_desc, \"menu_price\": menu_price}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"상세 정보 API 요청 실패: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fetch_price_data(self):\n",
    "        for cat_id in range(11, 16):\n",
    "            if cat_id == 12:\n",
    "                continue\n",
    "            url = f\"https://www.mcdelivery.co.kr/kr/browse/menu.html?daypartId=1&catId={cat_id}\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            script_content = soup.find_all('script', string=re.compile('ecommerce'))\n",
    "\n",
    "            if not script_content:\n",
    "                print(f\"catId {cat_id}: 데이터를 포함한 스크립트를 찾지 못했습니다.\")\n",
    "            else:\n",
    "                name_price_pattern = re.compile(r\"'name'\\s*:\\s*\\\"(.*?)\\\".*?'price'\\s*:\\s*'(\\d+)\", re.DOTALL)\n",
    "\n",
    "                for script in script_content:\n",
    "                    matches = name_price_pattern.findall(script.string)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            name, price = match\n",
    "                            clean_name = re.sub(r'\\s+', '', name)\n",
    "                            clean_name = self.clean_text(clean_name)\n",
    "                            self.menu_prices[clean_name] = price\n",
    "\n",
    "    def fetch_nutrition_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', class_='tableType01 nutrDesc')\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                nutrition_info = [[col.text.strip() for col in row.find_all(['th', 'td'])] for row in rows]\n",
    "                return nutrition_info\n",
    "            else:\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"영양 정보 API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_origin_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            origin_info_section = soup.find('ul', class_='origin_info')\n",
    "            if origin_info_section:\n",
    "                origin_info = origin_info_section.get_text(strip=True)\n",
    "                return re.sub(r'[™®]', '', origin_info)\n",
    "            else:\n",
    "                return \"원산지 정보 없음\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return \"원산지 정보 요청 실패\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        cleaned_text = re.sub(r'[™®]', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_nutrition_info(self, nutrition_data):\n",
    "        if not nutrition_data:\n",
    "            return \"영양소 정보 없음\"\n",
    "        headers = nutrition_data[0]\n",
    "        values = nutrition_data[1]\n",
    "        formatted_nutrition = {header: value for header, value in zip(headers, values) if header != \"영양소\"}\n",
    "        return formatted_nutrition\n",
    "\n",
    "    def save_menus_to_file_and_return_json(self, all_menu_items_by_category):\n",
    "        category_names = {\n",
    "            1: \"버거\",\n",
    "            7: \"사이드\",\n",
    "            8: \"디저트\",\n",
    "            9: \"맥카페\",\n",
    "            10: \"음료\",\n",
    "        }\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        today = datetime.now().strftime(\"%m%d\")\n",
    "        file_path = f\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_{today}_simple.json\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sub_category_seq, all_menu_items in all_menu_items_by_category.items():\n",
    "                category_name = category_names.get(sub_category_seq, \"기타\")\n",
    "                for item in all_menu_items:\n",
    "                    kor_name = self.clean_text(item.get('kor_name', ''))\n",
    "                    menu_code = item.get('seq', '')\n",
    "                    menu_details = self.fetch_menu_detail(menu_code, sub_category_seq)\n",
    "\n",
    "                    nutrition_data = self.format_nutrition_info(self.fetch_nutrition_data(menu_code, sub_category_seq))\n",
    "                    origin_info = self.fetch_origin_data(menu_code, sub_category_seq)\n",
    "\n",
    "                    document = {\n",
    "                        \"page_content\": f'{kor_name}{category_name}는 {menu_details[\"menu_desc\"]}이고, 식재료는 {origin_info}가 사용되었고, 가격은 {menu_details[\"menu_price\"]}원 입니다.',\n",
    "                        \"metadata\": {\n",
    "                            \"category\": category_name,\n",
    "                            \"name\": kor_name,\n",
    "                            \"price\": menu_details[\"menu_price\"],\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin_info\": origin_info\n",
    "                        }\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "\n",
    "            json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"메뉴 데이터를 파일에 저장했습니다.\")\n",
    "        return json.dumps(documents, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "menu = McDonaldsMenu()\n",
    "menu.fetch_price_data()\n",
    "\n",
    "all_menu_items_by_category = {}\n",
    "for sub_category_seq in [1, 7, 8, 9, 10]:\n",
    "    all_menu_items = []\n",
    "    for page_num in range(1, 6):\n",
    "        menu_data = menu.fetch_menu_data(sub_category_seq, page_num)\n",
    "        if not menu_data:\n",
    "            break\n",
    "        all_menu_items.extend(menu_data)\n",
    "    all_menu_items_by_category[sub_category_seq] = all_menu_items\n",
    "\n",
    "json_data = menu.save_menus_to_file_and_return_json(all_menu_items_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagecontent no price + Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가격 정보 누락: 맥윙™\n",
      "가격 정보 누락: 맥윙™콤보\n",
      "가격 정보 누락: 골든 모짜렐라 치즈스틱\n",
      "가격 정보 누락: 맥너겟®\n",
      "가격 정보 누락: 맥스파이시®치킨 텐더\n",
      "가격 정보 누락: 해쉬 브라운\n",
      "가격 정보 누락: 딸기 오레오 맥플러리\n",
      "가격 정보 누락: 초코 오레오 맥플러리\n",
      "가격 정보 누락: 스트로베리콘\n",
      "가격 정보 누락: 아이스크림콘\n",
      "가격 정보 누락: 바닐라 선데이 아이스크림\n",
      "가격 정보 누락: 초코 선데이 아이스크림\n",
      "가격 정보 누락: 딸기 선데이 아이스크림\n",
      "가격 정보 누락: 오레오 아포가토\n",
      "가격 정보 누락: 아이스 카페라떼\n",
      "가격 정보 누락: 디카페인 아이스 카페라떼\n",
      "가격 정보 누락: 아이스 아메리카노\n",
      "가격 정보 누락: 디카페인 아이스 아메리카노\n",
      "가격 정보 누락: 아이스 드립 커피\n",
      "가격 정보 누락: 바닐라 쉐이크 Medium\n",
      "가격 정보 누락: 딸기 쉐이크 Medium\n",
      "가격 정보 누락: 초코 쉐이크 Medium\n",
      "가격 정보 누락: 오렌지 주스\n",
      "메뉴 데이터를 파일에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "class McDonaldsMenu:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.mcdonalds.co.kr/kor/menu/listContent.do\"\n",
    "        self.detail_url = \"https://www.mcdonalds.co.kr/kor/menu/detail.do\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        self.menu_prices = {}\n",
    "\n",
    "    def fetch_menu_data(self, sub_category_seq, page_num):\n",
    "        data = {\n",
    "            \"page\": page_num,\n",
    "            \"sub_category_seq\": sub_category_seq\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('list', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_menu_detail(self, menu_code, sub_category_seq):\n",
    "        data = {\"seq\": menu_code, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            menu_name_tag = soup.find('h2', class_='ko')\n",
    "            menu_desc_tag = soup.find('div', class_='desc')\n",
    "\n",
    "            menu_name = menu_name_tag.get_text(strip=True) if menu_name_tag else \"메뉴 이름을 찾을 수 없음\"\n",
    "            cleaned_menu_name = re.sub(r'\\s+', '', menu_name)\n",
    "            cleaned_menu_name = self.clean_text(cleaned_menu_name)\n",
    "\n",
    "            menu_desc = menu_desc_tag.get_text(strip=True) if menu_desc_tag else \"메뉴 설명을 찾을 수 없음\"\n",
    "            menu_desc = re.sub(r'[™®]', '', menu_desc)\n",
    "            menu_desc = re.sub(r'\\*?판매\\s*시간\\s*[:：]?\\s*[^\\*]+', '', menu_desc).strip()\n",
    "\n",
    "            menu_price = self.menu_prices.get(cleaned_menu_name, \"가격 정보를 찾을 수 없음\")\n",
    "\n",
    "            if menu_price == \"가격 정보를 찾을 수 없음\":\n",
    "                print(f\"가격 정보 누락: {menu_name}\")\n",
    "\n",
    "            return {\"menu_name\": menu_name, \"menu_desc\": menu_desc, \"menu_price\": menu_price}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"상세 정보 API 요청 실패: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fetch_price_data(self):\n",
    "        for cat_id in range(11, 16):\n",
    "            if cat_id == 12:\n",
    "                continue\n",
    "            url = f\"https://www.mcdelivery.co.kr/kr/browse/menu.html?daypartId=1&catId={cat_id}\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            script_content = soup.find_all('script', string=re.compile('ecommerce'))\n",
    "\n",
    "            if not script_content:\n",
    "                print(f\"catId {cat_id}: 데이터를 포함한 스크립트를 찾지 못했습니다.\")\n",
    "            else:\n",
    "                name_price_pattern = re.compile(r\"'name'\\s*:\\s*\\\"(.*?)\\\".*?'price'\\s*:\\s*'(\\d+)\", re.DOTALL)\n",
    "\n",
    "                for script in script_content:\n",
    "                    matches = name_price_pattern.findall(script.string)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            name, price = match\n",
    "                            clean_name = re.sub(r'\\s+', '', name)\n",
    "                            clean_name = self.clean_text(clean_name)\n",
    "                            self.menu_prices[clean_name] = price\n",
    "\n",
    "    def fetch_nutrition_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', class_='tableType01 nutrDesc')\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                nutrition_info = [[col.text.strip() for col in row.find_all(['th', 'td'])] for row in rows]\n",
    "                return nutrition_info\n",
    "            else:\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"영양 정보 API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_origin_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            origin_info_section = soup.find('ul', class_='origin_info')\n",
    "            if origin_info_section:\n",
    "                origin_info = origin_info_section.get_text(strip=True)\n",
    "                return re.sub(r'[™®]', '', origin_info)\n",
    "            else:\n",
    "                return \"원산지 정보 없음\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return \"원산지 정보 요청 실패\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        cleaned_text = re.sub(r'[™®]', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_nutrition_info(self, nutrition_data):\n",
    "        if not nutrition_data:\n",
    "            return \"영양소 정보 없음\"\n",
    "        headers = nutrition_data[0]\n",
    "        values = nutrition_data[1]\n",
    "        formatted_nutrition = {header: value for header, value in zip(headers, values) if header != \"영양소\"}\n",
    "        return formatted_nutrition\n",
    "\n",
    "    def save_menus_to_file_and_return_json(self, all_menu_items_by_category):\n",
    "        category_names = {\n",
    "            1: \"버거\",\n",
    "            7: \"사이드\",\n",
    "            8: \"디저트\",\n",
    "            9: \"맥카페\",\n",
    "            10: \"음료\",\n",
    "        }\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        today = datetime.now().strftime(\"%m%d\")\n",
    "        file_path = f\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_{today}_no_price.json\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sub_category_seq, all_menu_items in all_menu_items_by_category.items():\n",
    "                category_name = category_names.get(sub_category_seq, \"기타\")\n",
    "                for item in all_menu_items:\n",
    "                    kor_name = self.clean_text(item.get('kor_name', ''))\n",
    "                    menu_code = item.get('seq', '')\n",
    "                    menu_details = self.fetch_menu_detail(menu_code, sub_category_seq)\n",
    "\n",
    "                    nutrition_data = self.format_nutrition_info(self.fetch_nutrition_data(menu_code, sub_category_seq))\n",
    "                    origin_info = self.fetch_origin_data(menu_code, sub_category_seq)\n",
    "\n",
    "                    document = {\n",
    "                        \"page_content\": f'{kor_name}{category_name}는 {menu_details[\"menu_desc\"]}이고, 식재료는 {origin_info}가 사용되었습니다.',\n",
    "                        \"metadata\": {\n",
    "                            \"category\": category_name,\n",
    "                            \"name\": kor_name,\n",
    "                            \"price\": menu_details[\"menu_price\"],\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin_info\": origin_info\n",
    "                        }\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "\n",
    "            json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"메뉴 데이터를 파일에 저장했습니다.\")\n",
    "        return json.dumps(documents, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "menu = McDonaldsMenu()\n",
    "menu.fetch_price_data()\n",
    "\n",
    "all_menu_items_by_category = {}\n",
    "for sub_category_seq in [1, 7, 8, 9, 10]:\n",
    "    all_menu_items = []\n",
    "    for page_num in range(1, 6):\n",
    "        menu_data = menu.fetch_menu_data(sub_category_seq, page_num)\n",
    "        if not menu_data:\n",
    "            break\n",
    "        all_menu_items.extend(menu_data)\n",
    "    all_menu_items_by_category[sub_category_seq] = all_menu_items\n",
    "\n",
    "json_data = menu.save_menus_to_file_and_return_json(all_menu_items_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagecontent만(no metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가격 정보 누락: 맥윙™\n",
      "가격 정보 누락: 맥윙™콤보\n",
      "가격 정보 누락: 골든 모짜렐라 치즈스틱\n",
      "가격 정보 누락: 맥너겟®\n",
      "가격 정보 누락: 맥스파이시®치킨 텐더\n",
      "가격 정보 누락: 해쉬 브라운\n",
      "가격 정보 누락: 딸기 오레오 맥플러리\n",
      "가격 정보 누락: 초코 오레오 맥플러리\n",
      "가격 정보 누락: 스트로베리콘\n",
      "가격 정보 누락: 아이스크림콘\n",
      "가격 정보 누락: 바닐라 선데이 아이스크림\n",
      "가격 정보 누락: 초코 선데이 아이스크림\n",
      "가격 정보 누락: 딸기 선데이 아이스크림\n",
      "가격 정보 누락: 오레오 아포가토\n",
      "가격 정보 누락: 아이스 카페라떼\n",
      "가격 정보 누락: 디카페인 아이스 카페라떼\n",
      "가격 정보 누락: 아이스 아메리카노\n",
      "가격 정보 누락: 디카페인 아이스 아메리카노\n",
      "가격 정보 누락: 아이스 드립 커피\n",
      "가격 정보 누락: 바닐라 쉐이크 Medium\n",
      "가격 정보 누락: 딸기 쉐이크 Medium\n",
      "가격 정보 누락: 초코 쉐이크 Medium\n",
      "가격 정보 누락: 오렌지 주스\n",
      "메뉴 데이터를 파일에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "class McDonaldsMenu:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.mcdonalds.co.kr/kor/menu/listContent.do\"\n",
    "        self.detail_url = \"https://www.mcdonalds.co.kr/kor/menu/detail.do\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "        self.menu_prices = {}\n",
    "\n",
    "    def fetch_menu_data(self, sub_category_seq, page_num):\n",
    "        data = {\n",
    "            \"page\": page_num,\n",
    "            \"sub_category_seq\": sub_category_seq\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get('list', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_menu_detail(self, menu_code, sub_category_seq):\n",
    "        data = {\"seq\": menu_code, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            menu_name_tag = soup.find('h2', class_='ko')\n",
    "            menu_desc_tag = soup.find('div', class_='desc')\n",
    "\n",
    "            menu_name = menu_name_tag.get_text(strip=True) if menu_name_tag else \"메뉴 이름을 찾을 수 없음\"\n",
    "            cleaned_menu_name = re.sub(r'\\s+', '', menu_name)\n",
    "            cleaned_menu_name = self.clean_text(cleaned_menu_name)\n",
    "\n",
    "            menu_desc = menu_desc_tag.get_text(strip=True) if menu_desc_tag else \"메뉴 설명을 찾을 수 없음\"\n",
    "            menu_desc = re.sub(r'[™®]', '', menu_desc)\n",
    "            menu_desc = re.sub(r'\\*?판매\\s*시간\\s*[:：]?\\s*[^\\*]+', '', menu_desc).strip()\n",
    "\n",
    "            menu_price = self.menu_prices.get(cleaned_menu_name, \"가격 정보를 찾을 수 없음\")\n",
    "\n",
    "            if menu_price == \"가격 정보를 찾을 수 없음\":\n",
    "                print(f\"가격 정보 누락: {menu_name}\")\n",
    "\n",
    "            return {\"menu_name\": menu_name, \"menu_desc\": menu_desc, \"menu_price\": menu_price}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"상세 정보 API 요청 실패: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def fetch_price_data(self):\n",
    "        for cat_id in range(11, 16):\n",
    "            if cat_id == 12:\n",
    "                continue\n",
    "            url = f\"https://www.mcdelivery.co.kr/kr/browse/menu.html?daypartId=1&catId={cat_id}\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            script_content = soup.find_all('script', string=re.compile('ecommerce'))\n",
    "\n",
    "            if not script_content:\n",
    "                print(f\"catId {cat_id}: 데이터를 포함한 스크립트를 찾지 못했습니다.\")\n",
    "            else:\n",
    "                name_price_pattern = re.compile(r\"'name'\\s*:\\s*\\\"(.*?)\\\".*?'price'\\s*:\\s*'(\\d+)\", re.DOTALL)\n",
    "\n",
    "                for script in script_content:\n",
    "                    matches = name_price_pattern.findall(script.string)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            name, price = match\n",
    "                            clean_name = re.sub(r'\\s+', '', name)\n",
    "                            clean_name = self.clean_text(clean_name)\n",
    "                            self.menu_prices[clean_name] = price\n",
    "\n",
    "    def fetch_nutrition_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', class_='tableType01 nutrDesc')\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')\n",
    "                nutrition_info = [[col.text.strip() for col in row.find_all(['th', 'td'])] for row in rows]\n",
    "                return nutrition_info\n",
    "            else:\n",
    "                return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"영양 정보 API 요청 실패: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_origin_data(self, seq, sub_category_seq):\n",
    "        data = {\"seq\": seq, \"page\": 1, \"sub_category_seq\": sub_category_seq}\n",
    "        try:\n",
    "            response = requests.post(self.detail_url, headers=self.headers, data=data)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            origin_info_section = soup.find('ul', class_='origin_info')\n",
    "            if origin_info_section:\n",
    "                origin_info = origin_info_section.get_text(strip=True)\n",
    "                return re.sub(r'[™®]', '', origin_info)\n",
    "            else:\n",
    "                return \"원산지 정보 없음\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return \"원산지 정보 요청 실패\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        cleaned_text = re.sub(r'[™®]', '', cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_nutrition_info(self, nutrition_data):\n",
    "        if not nutrition_data:\n",
    "            return \"영양소 정보 없음\"\n",
    "        headers = nutrition_data[0]\n",
    "        values = nutrition_data[1]\n",
    "        formatted_nutrition = {header: value for header, value in zip(headers, values) if header != \"영양소\"}\n",
    "        return formatted_nutrition\n",
    "\n",
    "    def save_menus_to_file_and_return_json(self, all_menu_items_by_category):\n",
    "        category_names = {\n",
    "            1: \"버거\",\n",
    "            7: \"사이드\",\n",
    "            8: \"디저트\",\n",
    "            9: \"맥카페\",\n",
    "            10: \"음료\",\n",
    "        }\n",
    "\n",
    "        documents = []\n",
    "\n",
    "        today = datetime.now().strftime(\"%m%d\")\n",
    "        file_path = f\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_{today}.json\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sub_category_seq, all_menu_items in all_menu_items_by_category.items():\n",
    "                category_name = category_names.get(sub_category_seq, \"기타\")\n",
    "                for item in all_menu_items:\n",
    "                    kor_name = self.clean_text(item.get('kor_name', ''))\n",
    "                    menu_code = item.get('seq', '')\n",
    "                    menu_details = self.fetch_menu_detail(menu_code, sub_category_seq)\n",
    "\n",
    "                    nutrition_data = self.format_nutrition_info(self.fetch_nutrition_data(menu_code, sub_category_seq))\n",
    "                    origin_info = self.fetch_origin_data(menu_code, sub_category_seq)\n",
    "\n",
    "                    document = {\n",
    "                        \"page_content\": {\n",
    "                            \"name\": kor_name,\n",
    "                            \"category\": category_name,\n",
    "                            \"description\": menu_details[\"menu_desc\"],\n",
    "                            \"price\": str(menu_details[\"menu_price\"]),  # 문자열로 변환\n",
    "                            \"nutrition\": nutrition_data,\n",
    "                            \"origin\": origin_info\n",
    "                        }\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "\n",
    "            json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"메뉴 데이터를 파일에 저장했습니다.\")\n",
    "        return json.dumps(documents, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "menu = McDonaldsMenu()\n",
    "menu.fetch_price_data()\n",
    "\n",
    "all_menu_items_by_category = {}\n",
    "for sub_category_seq in [1, 7, 8, 9, 10]:\n",
    "    all_menu_items = []\n",
    "    for page_num in range(1, 6):\n",
    "        menu_data = menu.fetch_menu_data(sub_category_seq, page_num)\n",
    "        if not menu_data:\n",
    "            break\n",
    "        all_menu_items.extend(menu_data)\n",
    "    all_menu_items_by_category[sub_category_seq] = all_menu_items\n",
    "\n",
    "json_data = menu.save_menus_to_file_and_return_json(all_menu_items_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
