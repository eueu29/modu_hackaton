import streamlit as st
from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from kiwipiepy import Kiwi
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from typing import List
from langchain_anthropic import ChatAnthropic
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda 
from langchain.memory import ConversationBufferMemory

# í™˜ê²½ì„¤ì •
load_dotenv()
logging.langsmith("Ensemble")

st.set_page_config(page_title="Ensemble chatbot", page_icon="ğŸ¤–")

st.title("ëª¨ë‘ì˜ì ì› McDonald orderbot")
st.markdown(
    """
    ë°˜ê°‘ìŠµë‹ˆë‹¤! ë§¥ë„ë‚ ë“œì—ì„œ í–‰ë³µí•œ ê²½í—˜ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
    """
)

# í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚½ì… ì‹œ ì—ëŸ¬ë°œìƒ, ë””ë²„ê¹…ì˜ˆì •
# kiwi = Kiwi()

# def kiwi_tokenize(text):
#     return [token.form for token in kiwi.tokenize(text)]


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])
            
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì„¤ì •

# Reset ë²„íŠ¼ ì¶”ê°€
if st.button("Reset"):
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì¬ì„¤ì •
    st.cache_resource.clear()
    st.experimental_rerun()
    
@st.cache_resource
def get_memory():
    return ConversationBufferMemory(
        llm=llm,
        max_token_limit=200,
        return_messages=True,
        memory_key="chat_history"
    )


@st.cache_resource  # st.cache_data ëŒ€ì‹  st.cache_resource ì‚¬ìš©
def embed_file(file_dir):
    loader = TextLoader(file_dir)
    docs = loader.load()
    
    file_name = file_dir.split("/")[-1]
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    
    # CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large"
    )

    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,
        document_embedding_cache=cache_dir,
        namespace="solar-embedding-1-large",  # Solar ì„ë² ë”© ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ namespace ë³€ê²½
    )

    faiss_vectorstore = FAISS.from_documents(
        split_docs,
        cached_embedder,
    )

    faiss = faiss_vectorstore.as_retriever(search_kwargs={"k": 4})
    
    kiwi_bm25 = BM25Retriever.from_documents(split_docs)
    # kiwi_bm25 = BM25Retriever.from_documents(split_docs, preprocess_func=kiwi_tokenize)
    kiwi_bm25.k = 4

    ensemble_retriever = EnsembleRetriever(
        retrievers=[kiwi_bm25, faiss],  # ì‚¬ìš©í•  ê²€ìƒ‰ ëª¨ë¸ì˜ ë¦¬ìŠ¤íŠ¸
        weights=[0.3, 0.7],  # ê° ê²€ìƒ‰ ëª¨ë¸ì˜ ê²°ê³¼ì— ì ìš©í•  ê°€ì¤‘ì¹˜
        search_type="mmr",  # ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ ì¦ì§„ì‹œí‚¤ëŠ” MMR ë°©ì‹ì„ ì‚¬ìš©
    )
    
    return ensemble_retriever

retriever = embed_file('/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_1002_noallergy.txt')

llm = ChatAnthropic(model_name="claude-3-5-sonnet-20240620")

# qa = RetrievalQA.from_chain_type(
#     llm = llm,
#     chain_type = "stuff",
#     retriever = retriever,
#     return_source_documents = True
# )

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    ë„ˆëŠ” ë§¥ë„ë‚ ë“œì˜ ìë™í™”ëœ ì£¼ë¬¸ì‹œìŠ¤í…œì´ë‹¤. 
    ê³ ê°ì˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì•„ë„ ìµœëŒ€í•œ ì¶”ì¸¡í•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì˜µì…˜ì„ ì œê³µí•´ì•¼ í•œë‹¤.
    ëª¨ë“  ì‘ë‹µì€ ì»¨í…ìŠ¤íŠ¸ì™€ ì±„íŒ… ê¸°ë¡ì— ê¸°ë°˜í•´ì•¼ í•œë‹¤.
    ë‹µë³€ì„ ëª¨ë¥´ëŠ” ê²½ìš° "ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”"ë¼ê³  ì‘ë‹µí•´ì•¼ í•œë‹¤.
    
    ì£¼ë¬¸ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:
    1. ê³ ê°ì˜ ì£¼ë¬¸ì„ ë°›ëŠ”ë‹¤. ê³ ê°ì´ ì£¼ë¬¸ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ê²½ìš°, ì‹ ì œí’ˆ ë©”ë‰´ë“¤ì„ ì¶”ì²œí•œë‹¤.
    2. ê³ ê°ì´ íŠ¹ë³„í•œ ì·¨í–¥ì´ ìˆëŠ” ê²½ìš°, ê·¸ì— ê°€ì¥ ê°€ê¹Œìš´ ë©”ë‰´ë¥¼ ì¶”ì²œí•œë‹¤.
    3. ì£¼ë¬¸ì´ ì™„ë£Œë˜ë©´, ì£¼ë¬¸ ìƒì„¸ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•œë‹¤.
    4. ë²„ê±° ë©”ë‰´ë¥¼ ì„ íƒí•  ë•ŒëŠ” í•­ìƒ ê³ ê°ì´ ì„¸íŠ¸ ë©”ë‰´ë¥¼ ì›í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.
    5. ì„¸íŠ¸ ë©”ë‰´ëŠ” ë²„ê±°, ì‚¬ì´ë“œ, ìŒë£Œê°€ í¬í•¨ëœë‹¤.
    6. ì£¼ë¬¸ì´ ì™„ë£Œë˜ë©´, ì „ì²´ ì£¼ë¬¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ê²€í† í•˜ê³  ê²°ì œ ë°©ë²•ì„ í™•ì¸í•œë‹¤.
    
    Context: {context}
    Chat History: {chat_history}
    """),
    ("human", "{question}"),
])


message = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

if message:
    send_message(message, "human", save=True)
    
    memory = get_memory()
    chain = {
    "context": retriever,
    "chat_history": RunnableLambda(lambda _: memory.chat_memory),
    "question": RunnablePassthrough()
} | prompt | llm
    
    with st.chat_message("ai"):
        response = chain.invoke(message)
        ai_response = response.content
        st.markdown(ai_response)
        
    memory.save_context({"input": message}, {"output": ai_response})
    st.experimental_rerun()
else:
    if st.session_state.get("first_encounter", False):
        st.session_state["messages"].append({"message": "ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”.", "role": "ai"})
        st.session_state["first_encounter"] = False  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ í•´ì œ
    paint_history()