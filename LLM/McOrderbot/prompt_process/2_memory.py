import streamlit as st
from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain.text_splitter import CharacterTextSplitter
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda 
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from datetime import datetime
from langchain_community.document_loaders import JSONLoader
import json
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever

# í™˜ê²½ì„¤ì •
load_dotenv()
logging.langsmith("TEST")

# Models
claude = ChatAnthropic(model_name="claude-3-5-sonnet-20240620")
gpt4o = ChatOpenAI(model_name="gpt-4o-2024-08-06") 
gpt4o_mini = ChatOpenAI(model_name="gpt-4o-mini-2024-07-18")
gpt_turbo = ChatOpenAI(model_name="gpt-3.5-turbo-0125")


st.set_page_config(page_title="Ensemble chatbot", page_icon="ğŸ¤–")

st.title("ëª¨ë‘ì˜ì ì› McDonald orderbot")
st.markdown(
    """
    ë°˜ê°‘ìŠµë‹ˆë‹¤! ë§¥ë„ë‚ ë“œì—ì„œ í–‰ë³µí•œ ê²½í—˜ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.  
    LLM ëª¨ë¸ì— template + RAG + memory ì‚¬ìš©ì¤‘ì…ë‹ˆë‹¤.  
    í˜„ì¬ ì‚¬ìš©ì¤‘ì¸ LLM ëª¨ë¸ì€ claude 3.5 Sonnet ì…ë‹ˆë‹¤.
    """
)

@st.cache_resource
def get_memory():
    return ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history"
    )
    
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])
            
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì„¤ì •

# Reset ë²„íŠ¼ ì¶”ê°€
if st.button("Reset"):
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì¬ì„¤ì •
    st.cache_resource.clear()
    # st.experimental_rerun() ëŒ€ì‹  st.rerun() ì‚¬ìš©
    st.rerun()

@st.cache_resource
def embed_file(file_dir):
    with open(file_dir, 'r', encoding='utf-8') as file:
        content = file.read()
        # JSON ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        json_objects = json.loads(content)

    # íŒŒì‹±ëœ JSON ê°ì²´ë“¤ì„ ë¬¸ì„œë¡œ ë³€í™˜
    docs = [
        Document(
            page_content=json.dumps(obj['page_content'], ensure_ascii=False),
        )
        for obj in json_objects
]

    
    file_name = file_dir.split("/")[-1]
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    
    # CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large"
    )
    
    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,
        document_embedding_cache=cache_dir,
        namespace="solar-embedding-1-large",
    )

    vectorstore = FAISS.from_documents(
        split_docs,
        cached_embedder,
    )

    faiss = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    bm25 = BM25Retriever.from_documents(split_docs)
    bm25.k = 4

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25, faiss],
        weights=[0.3, 0.7],
        search_type="mmr",
    )
    
    return ensemble_retriever

file_path ="/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_1017.json"
retriever = embed_file(file_path)

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    ë‹¹ì‹ ì€ ë§¥ë„ë‚ ë“œì˜ ì¹œì ˆí•œ ì ì›ì…ë‹ˆë‹¤.
    ê³ ê°ë‹˜ì˜ ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ì„¸ìš”. ë‚˜ì´ì™€ ìƒê´€ì—†ì´ 'ê³ ê°ë‹˜'ì´ë¼ê³ ë§Œ ë¶€ë¥´ê³ , ëª¨ë“  ì„¤ëª…ì€ ì–´ë¦°ì•„ì´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì„¸ìš”. 
    ëŒ€ë‹µì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ, ê°„ê²°í•˜ê³  ì¹œì ˆí•˜ê²Œ ì‘ëŒ€í•©ë‹ˆë‹¤.
    
    **ì‘ëŒ€ ì§€ì¹¨:**
    1. ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    2. ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì„ íƒí•´ ëŒ€ë‹µí•˜ì„¸ìš”.
    3. ë©”ë‰´ ì¶”ì²œì€ 2ê°œ ì´í•˜ë¡œ ì œí•œí•˜ë©°, ì‹ ë©”ë‰´ë¥¼ ìš°ì„  ì¶”ì²œí•˜ì„¸ìš”.
    4. í•­ìƒ ëŒ€í™” ê¸°ë¡ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•´ ëŒ€ë‹µí•˜ì„¸ìš”. íŠ¹íˆ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì§ˆë¬¸ì´ ë¶ˆë¶„ëª…í•  ê²½ìš° ì´ì „ ëŒ€í™”ì™€ ì—°ê²°ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    5. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤ë§Œ, ì¶”ê°€ë¡œ í™•ì¸ í›„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
    6. ì£¼ë¬¸ê³¼ ìƒê´€ì—†ëŠ” ì§ˆë¬¸ì„ í•  ê²½ìš°, ì‘ëŒ€ë¥¼ í•œ ë’¤ ë‹¤ì‹œ ì£¼ë¬¸ì„ ì´ì–´ê°€ë„ë¡ ìœ ë„í•˜ì„¸ìš”.

    **ì£¼ë¬¸ ì¡°ê±´:**
    - ë©”ë‰´ëŠ” ì„¸íŠ¸ì™€ ë‹¨í’ˆìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
    - ì„¸íŠ¸ë©”ë‰´ëŠ” ë²„ê±°, ì‚¬ì´ë“œ, ìŒë£Œê°€ í¬í•¨ë˜ë©°, ë¯¸ë””ì—„ ì‚¬ì´ì¦ˆê°€ ê¸°ë³¸ì…ë‹ˆë‹¤.
    - ë¯¸ë””ì—„ ì‚¬ì´ì¦ˆ ì„¸íŠ¸ë©”ë‰´ ê°€ê²©ì€ contextì˜ 'set_price' ê°€ê²©ì´ë©°, ì—†ì„ ê²½ìš° "ì£„ì†¡í•©ë‹ˆë‹¤, ì„¸íŠ¸ êµ¬ì„±ì´ ë¶ˆê°€ëŠ¥í•œ í•­ëª©ì…ë‹ˆë‹¤. ëŒ€ì‹  ë‹¨í’ˆìœ¼ë¡œ ì£¼ë¬¸í•˜ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë©”ë‰´ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”."ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
    - ë¼ì§€ ì‚¬ì´ì¦ˆ ì„¸íŠ¸ë©”ë‰´ë¡œì˜ ì—…ê·¸ë ˆì´ë“œëŠ” 800ì› ì¶”ê°€ ìš”ê¸ˆì´ ë¶€ê³¼ë©ë‹ˆë‹¤.
    - ì‚¬ì´ë“œëŠ” í›„ë Œì¹˜ í›„ë¼ì´ ë¯¸ë””ì—„ì´ ê¸°ë³¸ì´ë©°, ì½”ìš¸ìŠ¬ë¡œë¡œ ë¬´ë£Œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    - ìŒë£ŒëŠ” ì½”ì¹´ì½œë¼ ë¯¸ë””ì—„ì´ ê¸°ë³¸ì´ë©°, ìŒë£Œ ë³€ê²½ ì‹œ ì°¨ì•¡ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    **ì£¼ë¬¸ ì ˆì°¨:**
    - ê³ ê°ë‹˜ì˜ ì£¼ë¬¸ì„ ë°›ê³  ë©”ë‰´ê°€ ì™„ì„±ë˜ë©´ ì¶”ê°€ ì£¼ë¬¸ ì—¬ë¶€ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”.
    - ì£¼ë¬¸ì´ ë¯¸ì™„ë£Œëœ ê²½ìš° ì¶”ê°€ ì£¼ë¬¸ì„ ë¬»ì§€ ë§ˆì„¸ìš”.
    - ì£¼ë¬¸ì´ ì™„ë£Œë˜ë©´, ì£¼ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œí•©ë‹ˆë‹¤.
    
    **ì£¼ë¬¸ ê²°ê³¼ ì¶œë ¥:**
    - ëª¨ë“  LLM ì‘ë‹µì€ 'ì£¼ë¬¸ ê²°ê³¼ ì˜ˆì‹œ'ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
    - 'ì£¼ë¬¸ì™„ë£Œ', 'LLM ì‘ë‹µ', 'ì£¼ë¬¸ë‚´ì—­'ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

    **ì£¼ë¬¸ ê²°ê³¼ ì˜ˆì‹œ:**
    {{
        "completion": {{order_complete}},
        "reply" : "{{llm_response}}",
        "order" : 
        [
            [
                {{"name": "ë¶ˆê³ ê¸° ë²„ê±°", "num": "1", "price": "4200", "set_menu": True, "set_price": "6600"}},
                {{"name": "í›„ë Œì¹˜í›„ë¼ì´", "num": "1", "price": "0", "set_menu": True, "set_price": "0"}},
                {{"name": "ì½”ì¹´ì½œë¼", "num": "1", "price": "0", "set_menu": True, "set_price": "0"}},
                {{"name": "ìƒí•˜ì´ ì¹˜í‚¨ ìŠ¤ë‚µë©", "num": "1", "price": "4000", "set_menu": False}}
            ]
        ]
    }}
    
    **Context:** {context}
    **Chat History:** {chat_history}
    """),
    ("human", "{question}"),
])
        
message = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

if message:
    send_message(message, "human", save=True)
    
    memory = get_memory()
    chain = {
    "context": retriever,
    "chat_history": RunnableLambda(lambda _: memory.load_memory_variables({})["chat_history"]),
    "question": RunnablePassthrough()
    } | prompt | gpt4o
    
    with st.chat_message("ai"):
        response = chain.invoke(message)
        print(response)
        ai_response = response.content
        st.markdown(ai_response)
        
    memory.save_context({"input": message}, {"output": ai_response})
    save_message(ai_response, "ai")
    st.rerun()

else:
    if st.session_state.get("first_encounter", False):
        initial_message = "ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”."
        st.session_state["messages"].append({"message": initial_message, "role": "ai"})
        memory = get_memory()
        memory.save_context({"input": ""}, {"output": initial_message})
        st.session_state["first_encounter"] = False  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ í•´ì œ
    paint_history()