import streamlit as st
from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain.text_splitter import CharacterTextSplitter
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda 
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from datetime import datetime
from langchain_community.document_loaders import JSONLoader
import json
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever

# í™˜ê²½ì„¤ì •
load_dotenv()
logging.langsmith("TEST")

# Models
claude = ChatAnthropic(model_name="claude-3-5-sonnet-20240620")
gpt4o = ChatOpenAI(model_name="gpt-4o-2024-08-06") 
gpt4o_mini = ChatOpenAI(model_name="gpt-4o-mini-2024-07-18")
gpt_turbo = ChatOpenAI(model_name="gpt-3.5-turbo-0125")


st.set_page_config(page_title="Ensemble chatbot", page_icon="ğŸ¤–")

st.title("ëª¨ë‘ì˜ì ì› McDonald orderbot")
st.markdown(
    """
    ë°˜ê°‘ìŠµë‹ˆë‹¤! ë§¥ë„ë‚ ë“œì—ì„œ í–‰ë³µí•œ ê²½í—˜ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.  
    LLM ëª¨ë¸ì— ê¸°ë³¸ Template + RAG ì‚¬ìš©ì¤‘ì…ë‹ˆë‹¤.  
    í˜„ì¬ ì‚¬ìš©ì¤‘ì¸ LLM ëª¨ë¸ì€ claude 3.5 Sonnet ì…ë‹ˆë‹¤.
    """
)

def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])
            
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì„¤ì •

# Reset ë²„íŠ¼ ì¶”ê°€
if st.button("Reset"):
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì¬ì„¤ì •
    st.cache_resource.clear()
    # st.experimental_rerun() ëŒ€ì‹  st.rerun() ì‚¬ìš©
    st.rerun()

@st.cache_resource
def embed_file(file_dir):
    with open(file_dir, 'r', encoding='utf-8') as file:
        content = file.read()
        # JSON ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        json_objects = json.loads(content)

    # íŒŒì‹±ëœ JSON ê°ì²´ë“¤ì„ ë¬¸ì„œë¡œ ë³€í™˜
    docs = [
        Document(
            page_content=json.dumps(obj['page_content'], ensure_ascii=False),
        )
        for obj in json_objects
]

    
    file_name = file_dir.split("/")[-1]
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    
    # CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large"
    )
    
    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,
        document_embedding_cache=cache_dir,
        namespace="solar-embedding-1-large",
    )

    vectorstore = FAISS.from_documents(
        split_docs,
        cached_embedder,
    )

    faiss = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    bm25 = BM25Retriever.from_documents(split_docs)
    bm25.k = 4

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25, faiss],
        weights=[0.3, 0.7],
        search_type="mmr",
    )
    
    return ensemble_retriever

file_path ="/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_1017.json"
retriever = embed_file(file_path)

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    ë‹¹ì‹ ì€ ë§¥ë„ë‚ ë“œì˜ ì¹œì ˆí•œ ì ì›ì…ë‹ˆë‹¤.
    ê³ ê°ë‹˜ì˜ ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ì„¸ìš”. ë‚˜ì´ì™€ ìƒê´€ì—†ì´ 'ê³ ê°ë‹˜'ì´ë¼ê³ ë§Œ ë¶€ë¥´ê³ , ëª¨ë“  ì„¤ëª…ì€ ì–´ë¦°ì•„ì´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì„¸ìš”. 
    ëŒ€ë‹µì€ ìµœëŒ€í•œ ê°„ê²°í•˜ê³  ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ì‘ëŒ€í•©ë‹ˆë‹¤.
    
    ëŒ€ë‹µí•  ë•ŒëŠ” ë‹¤ìŒ ì‚¬í•­ì„ ê¸°ì–µí•˜ì„¸ìš”:
    1. ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    2. ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ë‹µë³€í•˜ì„¸ìš”.
    3. ë©”ë‰´ ì¶”ì²œì€ 2ê°œ ì´í•˜ë¡œ ì œí•œí•˜ê³ , ì‹ ë©”ë‰´ë¥¼ ë¨¼ì € ì¶”ì²œí•˜ì„¸ìš”.
    4. í•­ìƒ ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•´ ëŒ€ë‹µí•˜ì„¸ìš”.
    5. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.

    **ì£¼ë¬¸ ì¡°ê±´:**
    - ë©”ë‰´ëŠ” ì„¸íŠ¸ì™€ ë‹¨í’ˆìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.
    - ì„¸íŠ¸ë©”ë‰´ëŠ” ë²„ê±°, ì‚¬ì´ë“œ, ìŒë£Œê°€ í¬í•¨ë˜ë©°, ë¯¸ë””ì—„ ì‚¬ì´ì¦ˆê°€ ê¸°ë³¸ì…ë‹ˆë‹¤.
    - ì‚¬ì´ì¦ˆ ì—…ê·¸ë ˆì´ë“œëŠ” 800ì› ì¶”ê°€ ìš”ê¸ˆì´ ë¶€ê³¼ë©ë‹ˆë‹¤.
    - ì‚¬ì´ë“œëŠ” í›„ë Œì¹˜ í›„ë¼ì´ ë¯¸ë””ì—„ì´ ê¸°ë³¸ì´ë©°, ì½”ìš¸ìŠ¬ë¡œë¡œ ë¬´ë£Œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    - ìŒë£ŒëŠ” ì½”ì¹´ì½œë¼ ë¯¸ë””ì—„ì´ ê¸°ë³¸ì´ë©°, ìŒë£Œ ë³€ê²½ ì‹œ ì°¨ì•¡ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì„¸íŠ¸ë©”ë‰´ ê°€ê²©ì€ 'set_burger_price'ì— 5600ì›ì„ ë”í•œ ê¸ˆì•¡ì…ë‹ˆë‹¤.
    - ë‹¨í’ˆ í›„ë Œì¹˜ í›„ë¼ì´ëŠ” ìŠ¤ëª°(2300ì›), ë¯¸ë””ì—„(3000ì›), ë¼ì§€(4000ì›) ì¤‘ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ë‹¨í’ˆ ìŒë£ŒëŠ” ë¯¸ë””ì—„(2600ì›) ë˜ëŠ” ë¼ì§€(3100ì›)ì…ë‹ˆë‹¤.
    
    **ì£¼ë¬¸ ì ˆì°¨:**
    - ë¨¼ì € ê³ ê°ì˜ ì£¼ë¬¸ì„ ë°›ê³  ë©”ë‰´ê°€ ì™„ì„±ë˜ë©´ ì£¼ë¬¸ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í•œë²ˆë§Œ ë¬¼ì–´ë³´ì„¸ìš”.
    - ë©”ë‰´ì˜ ì£¼ë¬¸ì´ ì™„ì„±ë˜ì§€ ì•Šì€ ê²½ìš° ì£¼ë¬¸ì™„ë£Œê°€ ë˜ì—ˆëŠ”ì§€ ë¬»ì§€ ë§ˆì„¸ìš”.
    - ì£¼ë¬¸ì´ ëª¨ë‘ ì™„ë£Œë˜ë©´ ì£¼ë¬¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
            
    context: {context}
    """),
    ("human", "{question}"),
])

message = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

if message:
    send_message(message, "human", save=True)
    
    chain = {
        "context": retriever,
        "question": RunnablePassthrough()
    } | prompt | claude
        
    with st.chat_message("ai"):
        response = chain.invoke(message)
        ai_response = response.content
        st.markdown(ai_response)
        
    save_message(ai_response, "ai")
    st.rerun()

else:
    if st.session_state.get("first_encounter", False):
        initial_message = "ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”."
        st.session_state["messages"].append({"message": initial_message, "role": "ai"})
        st.session_state["first_encounter"] = False  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ í•´ì œ
    paint_history()