{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "prompt_chaining\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"prompt_chaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt chain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_order_output(output: str) -> dict:\n",
    "    try:\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\n",
    "            \"주문_내용\": [],\n",
    "            \"수량\": [],\n",
    "            \"특별_요청\": [\"주문 정보를 파싱할 수 없습니다.\"]\n",
    "        }\n",
    "\n",
    "def create_chains(model):\n",
    "    understand_order = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        고객의 주문을 이해하고 필요한 정보를 추출하세요. 주문 내용, 수량, 특별 요청 사항 등을 파악하세요.\n",
    "        \n",
    "        이전 대화 내역:\n",
    "        {chat_history}\n",
    "        \n",
    "        출력 형식:\n",
    "        {{\n",
    "            \"주문_내용\": [주문 항목들],\n",
    "            \"수량\": [각 항목의 수량],\n",
    "            \"특별_요청\": [특별 요청 사항]\n",
    "        }}\n",
    "        \n",
    "        반드시 위의 출력 형식에 맞춰 JSON 형태로 응답해주세요. 앞뒤에 ```json과 같은 마크다운 표시를 하지 마세요.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]) | model | StrOutputParser()\n",
    "\n",
    "\n",
    "    check_menu = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        주문 내용을 확인하고 메뉴에 있는지 검증하세요. 가격과 세트 여부도 확인하세요.\n",
    "        \n",
    "        출력 형식:\n",
    "        {{\n",
    "            \"확인된_메뉴\": [확인된 메뉴 항목들],\n",
    "            \"가격\": [각 항목의 가격],\n",
    "            \"세트_여부\": [각 항목의 세트 여부]\n",
    "        }}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]) | model | StrOutputParser()\n",
    "\n",
    "    suggest_additions = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        현재 주문에 추가할 만한 메뉴를 추천하세요. 세트 메뉴 업그레이드나 사이드 메뉴 추가 등을 제안하세요.\n",
    "        \n",
    "        출력 형식:\n",
    "        {{\n",
    "            \"추천_메뉴\": [추천 메뉴 항목들],\n",
    "            \"추천_이유\": [각 추천 항목의 이유]\n",
    "        }}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]) | model | StrOutputParser()\n",
    "\n",
    "    summarize_order = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        전체 주문 내용을 요약하고 최종 가격을 계산하세요. 주문 완료 여부도 결정하세요.\n",
    "        \n",
    "        출력 형식:\n",
    "        {{\n",
    "            \"주문_요약\": [주문 항목 요],\n",
    "            \"총_가격\": 총 가격,\n",
    "            \"주문_완료\": true/false\n",
    "        }}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]) | model | StrOutputParser()\n",
    "\n",
    "    return {\n",
    "        \"understand_order\": understand_order,\n",
    "        \"check_menu\": check_menu,\n",
    "        \"suggest_additions\": suggest_additions,\n",
    "        \"summarize_order\": summarize_order\n",
    "    }\n",
    "\n",
    "def invoke(question, model, memory, retriever):\n",
    "    context = retriever.invoke(question)\n",
    "    context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "    chains = create_chains(model)\n",
    "\n",
    "    # 단계별 실행\n",
    "    understood_order = chains[\"understand_order\"].invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    checked_menu = chains[\"check_menu\"].invoke({\n",
    "        \"input\": understood_order,\n",
    "        \"context\": context_str\n",
    "    })\n",
    "    \n",
    "    suggestions = chains[\"suggest_additions\"].invoke({\n",
    "        \"input\": checked_menu,\n",
    "        \"context\": context_str\n",
    "    })\n",
    "    \n",
    "    # 사용자에게 추천 메뉴 확인\n",
    "    print(f\"AI: {suggestions}와 같은 메뉴를 추가로 추천드립니다. 추가하시겠어요? (예/아니오)\")\n",
    "    user_addition = input(\"고객: \").strip().lower()\n",
    "    \n",
    "    if user_addition == '예':\n",
    "        print(\"AI: 어떤 메뉴를 추가하시겠어요?\")\n",
    "        additional_order = input(\"고객: \").strip()\n",
    "        checked_menu += f\"\\n추가 주문: {additional_order}\"\n",
    "\n",
    "    order_summary = chains[\"summarize_order\"].invoke({\n",
    "        \"input\": f\"{checked_menu}\\n{suggestions}\",\n",
    "        \"context\": context_str\n",
    "    })\n",
    "\n",
    "    # 최종 주문 확인\n",
    "    print(f\"AI: 최종 주문 내역입니다: {order_summary}\")\n",
    "    print(\"이대로 주문하시겠어요? (예/아니오)\")\n",
    "    final_confirmation = input(\"고객: \").strip().lower()\n",
    "\n",
    "    if final_confirmation != '예':\n",
    "        return {\"전송\": True, \"응답\": \"주문을 처음부터 다시 시작하겠습니다.\"}\n",
    "\n",
    "    # 최종 응답 생성\n",
    "    final_response = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        주문 내용을 바탕으로 고객에게 친절하게 응답하세요. 주문 확인, 추천 사항, 최종 가격을 포함하세요.\n",
    "        \n",
    "        **주문 결과 예시:**\n",
    "        {{\n",
    "            \"전송\": true,\n",
    "            \"응답\": \"{{llm_response}}\"\n",
    "        }}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{order_summary}\")\n",
    "    ]) | model | StrOutputParser()\n",
    "\n",
    "    return final_response.invoke({\"order_summary\": order_summary})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하나씩 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(file_dir):\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=json.dumps(obj['page_content'], ensure_ascii=False),\n",
    "        )\n",
    "        for obj in json.load(open(file_dir, 'r', encoding='utf-8'))\n",
    "    ]\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=0)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file_dir.split('/')[-1]}\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings=embeddings,\n",
    "        document_embedding_cache=cache_dir,\n",
    "        namespace=\"solar-embedding-1-large\",\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(split_docs, cached_embedder)\n",
    "    faiss = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    bm25 = BM25Retriever.from_documents(split_docs)\n",
    "    bm25.k = 2\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25, faiss],\n",
    "        weights=[0.3, 0.7],\n",
    "        search_type=\"mmr\",\n",
    "    )\n",
    "    return ensemble_retriever\n",
    "\n",
    "retriever = create_retriever(\"/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_1017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_mini = ChatOpenAI(model_name=\"gpt-4o-mini-2024-07-18\", temperature=0.3)\n",
    "gpt4o = ChatOpenAI(model_name=\"gpt-4o-2024-08-06\") \n",
    "claude = ChatAnthropic(model_name=\"claude-3-5-sonnet-20240620\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "def save_context(user_message, ai_message):\n",
    "    memory.save_context({\"input\": str(user_message)}, {\"output\": str(ai_message)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain #1 Understand order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand_order = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    이전 대화내역을 참고해 고객의 주문을 이해하고 필요한 정보를 추출하세요. \n",
    "    주문 내용, 수량, 특별 요청 사항 등을 파악하세요.\n",
    "    특별 요청 사항에 포함되는 항목은 주문 내용에서 제외하세요.\n",
    "    \n",
    "    \n",
    "    이전 대화 내역:\n",
    "    {chat_history}\n",
    "    \n",
    "    출력 형식:\n",
    "    {{\n",
    "        \"주문_내용\": [주문 항목들],\n",
    "        \"수량\": [각 항목의 수량],\n",
    "        \"특별_요청\": [특별 요청 사항]\n",
    "    }}\n",
    "    \n",
    "    반드시 위의 출력 형식에 맞춰 JSON 형태로 응답해주세요. 앞뒤에 ```json과 같은 마크다운 표시를 하지 마세요.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) | gpt4o_mini | StrOutputParser()\n",
    "\n",
    "chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "question =  \"매운거 잘 못먹는데 버거중에 추천해줄만한거 있을까?\"\n",
    "\n",
    "understand_order_r = understand_order.invoke({\n",
    "    \"input\": question,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "understand_order_r = json.loads(understand_order_r)\n",
    "\n",
    "understand_r = {\"understand_order\": understand_order_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'주문_내용': [], '수량': [], '특별_요청': ['매운 음식은 피하고 싶음']}\n"
     ]
    }
   ],
   "source": [
    "print(understand_order_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand_order = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    이전 대화내역을 참고해 고객의 주문을 이해하고 필요한 정보를 추출하세요. \n",
    "    주문 내용, 수량, 특별 요청 사항 등을 파악하세요.\n",
    "    특별 요청 사항에 포함되는 항목은 주문 내용에서 제외하세요.\n",
    "    \n",
    "    \n",
    "    이전 대화 내역:\n",
    "    {chat_history}\n",
    "    \n",
    "    출력 형식:\n",
    "    {{\n",
    "        \"주문_내용\": [주문 항목들],\n",
    "        \"수량\": [각 항목의 수량],\n",
    "        \"특별_요청\": [특별 요청 사항]\n",
    "    }}\n",
    "    \n",
    "    반드시 위의 출력 형식에 맞춰 JSON 형태로 응답해주세요. 앞뒤에 ```json과 같은 마크다운 표시를 하지 마세요.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) | gpt4o_mini | StrOutputParser()\n",
    "\n",
    "chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "question =  \"스낵랩도 하나 주고,세트로 불고기버거 미디엄사이즈로 줘 \"\n",
    "\n",
    "understand_order_r = understand_order.invoke({\n",
    "    \"input\": question,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "understand_order_r = json.loads(understand_order_r)\n",
    "\n",
    "understand_r = {\"understand_order\": understand_order_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'주문_내용': ['스낵랩', '불고기버거 세트'], '수량': [1, 1], '특별_요청': ['불고기버거 미디엄 사이즈']}\n"
     ]
    }
   ],
   "source": [
    "print(understand_order_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['스낵랩', '불고기버거 세트']\n",
      "주문 내용: 스낵랩, 불고기버거 세트\n"
     ]
    }
   ],
   "source": [
    "result = understand_r[\"understand_order\"]\n",
    "print(result['주문_내용'])\n",
    "str_result = f\"주문 내용: {', '.join(result['주문_내용'])}\"\n",
    "print(str_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주문 내용: 스낵랩, 불고기버거 세트, 수량: 1, 1, 특별 요청: 불고기버거 미디엄 사이즈\n"
     ]
    }
   ],
   "source": [
    "str_result = f\"주문 내용: {', '.join(result['주문_내용'])}, 수량: {', '.join(map(str, result['수량']))}, 특별 요청: {', '.join(result['특별_요청'])}\"\n",
    "print(str_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain #2 check_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_menu = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    주문 내용을 확인하고 메뉴정보에 있는지 검증하세요. 가격과 세트 여부도 확인하세요.\n",
    "    불확실한 메뉴명이었으나 메뉴정보를 참고한 후 확인된 메뉴 항목은 \"확인된_메뉴\"에 포함시키고 \"확인되지_않은_메뉴\"에는 포함시키지 마세요.\n",
    "    해결되지 못한 특별요청은 확인되지_않은_이유에 기술하세요\n",
    "    가격, 세트여부가 확인되지 않은 메뉴는 \"확인되지_않은_메뉴\"에 포함시키고 가격, 세트여부에 관련 정보를 삽입하지 마세요.\n",
    "    \n",
    "    <메뉴정보>\n",
    "    {context}\n",
    "    </메뉴정보>\n",
    "    \n",
    "    출력 형식:\n",
    "    {{\n",
    "        \"확인된_메뉴\": [확인된 메뉴 항목들],\n",
    "        \"가격\": [각 항목의 가격],\n",
    "        \"세트_여부\": [각 항목의 세트 여부],\n",
    "        \"확인되지_않은_메뉴\" : [확인되지 않은 메뉴 항목들],\n",
    "        \"확인되지_않은_이유\":[간략하게 확인되지 않는 이유]\n",
    "    }}\n",
    "    반드시 위의 출력 형식에 맞춰 JSON 형태로 응답해주세요. 앞뒤에 ```json과 같은 마크다운 표시를 하지 마세요.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) | gpt4o | StrOutputParser()\n",
    "\n",
    "result1 = understand_r[\"understand_order\"]\n",
    "question_str = f\"주문 내용: {', '.join(result1['주문_내용'])}, 특별 요청: {', '.join(result1['특별_요청'])}\"\n",
    "\n",
    "context = retriever.invoke(question_str)\n",
    "context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "checked_menu = check_menu.invoke({\n",
    "    \"input\": result1,\n",
    "    \"context\": context_str\n",
    "})\n",
    "\n",
    "checked_menu_r = json.loads(checked_menu)\n",
    "\n",
    "check_r = {\"check_menu\" : checked_menu_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'확인된_메뉴': [], '가격': [], '세트_여부': [], '확인되지_않은_메뉴': [], '확인되지_않은_이유': ['매운 음식 피하기 요청에 해당하는 메뉴를 식별할 수 없음']}\n"
     ]
    }
   ],
   "source": [
    "print(checked_menu_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain #3 suggest_additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_additions = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    현재 주문에 추가할 만한 메뉴를 추천하세요. 세트 메뉴 업그레이드나 사이드 메뉴 추가 등을 제안하세요.\n",
    "    \n",
    "    세트메뉴는 \n",
    "    \n",
    "    출력 형식:\n",
    "    {\n",
    "        \"추천_메뉴\": [추천 메뉴 항목들],\n",
    "        \"추천_이유\": [각 추천 항목의 이유]\n",
    "    }\n",
    "    반드시 위의 출력 형식에 맞춰 JSON 형태로 응답해주세요. 앞뒤에 ```json과 같은 마크다운 표시를 하지 마세요\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) | gpt4o_mini | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "def save_context(user_message, ai_message):\n",
    "    memory.save_context({\"input\": str(user_message)}, {\"output\": str(ai_message)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_context(\"매콤한거 먹고싶어\",\"스리라차 마요버거와 맥크리스피 스파이시 버거를 추천드릴 수 있어요\")\n",
    "save_context(\"둘중에 뭐가 더 싸?\", \"가격이 저렴한 것을 찾으신다면 맥크리스피 스파이시 버거가 6700원으로 더 저렴합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_menu(question):\n",
    "    chat_history = memory.load_memory_variables(['chat_history'])\n",
    "    \n",
    "    guess_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "        \"\"\"\n",
    "        Chat History을 참고해 고객의 주문을 이해하고 메뉴명을 파악하세요\n",
    "        \n",
    "        출력형식: \"빅맥\"\n",
    "        \n",
    "        반드시 위의 출력 형식에 맞춰 메뉴명만 출력하세요.\n",
    "        **Chat History:** {chat_history}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]) | gpt4o_mini | StrOutputParser()\n",
    "\n",
    "    guess_chain = guess_template.invoke({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    return guess_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맥크리스피 스파이시 버거'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_menu(\"그거로 줘\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
