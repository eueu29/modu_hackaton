import streamlit as st
from dotenv import load_dotenv
from langchain_teddynote import logging
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from kiwipiepy import Kiwi
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain_upstage import UpstageEmbeddings
from typing import List
from langchain_anthropic import ChatAnthropic
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda 
from langchain.memory import ConversationBufferMemory

# í™˜ê²½ì„¤ì •
load_dotenv()
logging.langsmith("Ensemble")

st.set_page_config(page_title="Ensemble chatbot", page_icon="ğŸ¤–")

st.title("ëª¨ë‘ì˜ì ì› McDonald orderbot")
st.markdown(
    """
    ë°˜ê°‘ìŠµë‹ˆë‹¤! ë§¥ë„ë‚ ë“œì—ì„œ í–‰ë³µí•œ ê²½í—˜ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
    """
)

# í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚½ì… ì‹œ ì—ëŸ¬ë°œìƒ, ë””ë²„ê¹…ì˜ˆì •
# kiwi = Kiwi()

# def kiwi_tokenize(text):
#     return [token.form for token in kiwi.tokenize(text)]


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])
            
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì„¤ì •

# Reset ë²„íŠ¼ ì¶”ê°€
if st.button("Reset"):
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì¬ì„¤ì •
    st.cache_resource.clear()
    # st.experimental_rerun() ëŒ€ì‹  st.rerun() ì‚¬ìš©
    st.rerun()

@st.cache_resource
def get_memory():
    return ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history"
    )

@st.cache_resource  # st.cache_data ëŒ€ì‹  st.cache_resource ì‚¬ìš©
def embed_file(file_dir):
    loader = TextLoader(file_dir)
    docs = loader.load()
    
    file_name = file_dir.split("/")[-1]
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    
    # CharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large"
    )

    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,
        document_embedding_cache=cache_dir,
        namespace="solar-embedding-1-large",  # Solar ì„ë² ë”© ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ namespace ë³€ê²½
    )

    faiss_vectorstore = FAISS.from_documents(
        split_docs,
        cached_embedder,
    )

    faiss = faiss_vectorstore.as_retriever(search_kwargs={"k": 4})
    
    kiwi_bm25 = BM25Retriever.from_documents(split_docs)
    # kiwi_bm25 = BM25Retriever.from_documents(split_docs, preprocess_func=kiwi_tokenize)
    kiwi_bm25.k = 4

    ensemble_retriever = EnsembleRetriever(
        retrievers=[kiwi_bm25, faiss],  # ì‚¬ìš©í•  ê²€ìƒ‰ ëª¨ë¸ì˜ ë¦¬ìŠ¤íŠ¸
        weights=[0.3, 0.7],  # ê° ê²€ìƒ‰ ëª¨ë¸ì˜ ê²°ê³¼ì— ì ìš©í•  ê°€ì¤‘ì¹˜
        search_type="mmr",  # ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ ì¦ì§„ì‹œí‚¤ëŠ” MMR ë°©ì‹ì„ ì‚¬ìš©
    )
    
    return ensemble_retriever

retriever = embed_file('/home/yoojin/ML/aiffel/HackaThon/modu_hackaton/LLM/files/menu_1008.txt')

llm = ChatAnthropic(model_name="claude-3-5-sonnet-20240620")

# qa = RetrievalQA.from_chain_type(
#     llm = llm,
#     chain_type = "stuff",
#     retriever = retriever,
#     return_source_documents = True
# )

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    ë‹¹ì‹ ì€ ë§¥ë„ë‚ ë“œ ê°€ê²Œì˜ ì ì›ì…ë‹ˆë‹¤. 
    ë‚˜ì´ê°€ ë§ì€ ë…¸ì¸ ê³ ê°ì˜ ì£¼ë¬¸ì„ ë„ì™€ì£¼ì„¸ìš”.
    ë‚˜ì´ì™€ ê´€ë ¨ëœ ì–´ë–¤í•œ í˜¸ì¹­ë„ í•˜ì§€ ë§ê³  'ê³ ê°ë‹˜'ìœ¼ë¡œ ë¶€ë¥´ì„¸ìš”.
    ì–´ë¦°ì•„ì´ë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹¨ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ìƒëƒ¥í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ëŒ€í•´ì£¼ì„¸ìš”.
    ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ë©°, ê°€ëŠ¥í•œ í•œ ë¬¸ì¥ ì´ë‚´ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.  
    
    ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í•œ ê²½ìš°, "ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ì£¼ì„¸ìš”."ë¼ê³  ë‹µí•˜ì„¸ìš”.
    
    ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì„¸ìš”.
    ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ë‹µë³€í•˜ì„¸ìš”.
    ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”.

    ë‹µë³€ì´ í™•ì‹¤í•˜ì§€ ì•Šì„ ê²½ìš°, "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
    
    ì£¼ë¬¸ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    1. ê³ ê°ì˜ ì£¼ë¬¸ì„ ë°›ìŠµë‹ˆë‹¤.
    2. ë²„ê±° ë©”ë‰´ë¥¼ ì„ íƒí•  ë•Œ í•­ìƒ ì„¸íŠ¸ ë©”ë‰´ë¥¼ ì›í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. 
       ì„¸íŠ¸ ë©”ë‰´ëŠ” ë²„ê±°, ì‚¬ì´ë“œ, ìŒë£Œê°€ í¬í•¨ë©ë‹ˆë‹¤. 
       ê¸°ë³¸ ì‚¬ì´ë“œëŠ” í”„ë Œì¹˜í”„ë¼ì´ 1ê°œ, ìŒë£ŒëŠ” ì½œë¼ 1ê°œì…ë‹ˆë‹¤. ì‚¬ì´ë“œì™€ ìŒë£ŒëŠ” ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.
       ë‹¤ë¥¸ ë©”ë‰´ë¡œ ë³€ê²½ ì‹œ ì¶”ê°€ ê¸ˆì•¡ì„ ì•ˆë‚´í•´ ë“œë¦½ë‹ˆë‹¤.
    3. ì¶”ê°€ ì£¼ë¬¸ì´ í•„ìš”í•œì§€ ë¬»ê³ , ì¶”ê°€ ì£¼ë¬¸ì´ ì—†ìœ¼ë©´ ìµœì¢… ì£¼ë¬¸ì„ ë°›ìŠµë‹ˆë‹¤.
    4. ì£¼ë¬¸ ì™„ë£Œ ì‹œ ì „ì²´ ì£¼ë¬¸ì„ ê²€í† í•˜ê³  ê²°ì œ ë°©ë²•ì„ í™•ì¸í•©ë‹ˆë‹¤.
    5. ì£¼ë¬¸ ê²°ê³¼ë¥¼ json í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    ì£¼ë¬¸ ê²°ê³¼ ì˜ˆì‹œ: 
    {{
        "ì£¼ë¬¸ ë©”ë‰´" : [
            {{
                "ë©”ë‰´ ì´ë¦„" : "í–„ë²„ê±°",
                "ì¶”ê°€ ì˜µì…˜" : "í”„ë Œì¹˜í”„ë¼ì´ 1ê°œ, ì½œë¼ 1ê°œ",
                "ë‹¨í’ˆ ê¸ˆì•¡" : 0,
                "ì¶”ê°€ ê¸ˆì•¡" : 0,
                "ì£¼ë¬¸ ê¸ˆì•¡" : 0
            }},
            {{
                "ë©”ë‰´ ì´ë¦„" : "ì¹˜í‚¨ë²„ê±°",
                "ì¶”ê°€ ì˜µì…˜" : "í”„ë Œì¹˜í”„ë¼ì´ 1ê°œ, í™˜íƒ€ 1ê°œ",
                "ë‹¨í’ˆ ê¸ˆì•¡" : 0,
                "ì¶”ê°€ ê¸ˆì•¡" : 0,
                "ì£¼ë¬¸ ê¸ˆì•¡" : 0
            }}
        ],
        "ì´ ì£¼ë¬¸ ê¸ˆì•¡" : 0,
        "ê²°ì œ ë°©ë²•" : "ì¹´ë“œ"
    }}

    context: {context}
    chat history: {chat_history}
    """),
    ("human", "{question}"),
])

message = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

if message:
    send_message(message, "human", save=True)
    
    memory = get_memory()
    chain = {
        "context": retriever,
        "chat_history": RunnableLambda(lambda _: memory.load_memory_variables({})["chat_history"]),
        "question": RunnablePassthrough()
    } | prompt | llm
    
    with st.chat_message("ai"):
        response = chain.invoke(message)
        ai_response = response.content
        st.markdown(ai_response)
        
    memory.save_context({"input": message}, {"output": ai_response})
    save_message(ai_response, "ai")
    st.rerun()

else:
    if st.session_state.get("first_encounter", False):
        initial_message = "ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”."
        st.session_state["messages"].append({"message": initial_message, "role": "ai"})
        memory = get_memory()
        memory.save_context({"input": ""}, {"output": initial_message})
        st.session_state["first_encounter"] = False  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ í•´ì œ
    paint_history()