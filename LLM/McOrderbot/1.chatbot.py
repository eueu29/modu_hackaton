import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda 
from langchain.callbacks.base import BaseCallbackHandler
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
from langchain.callbacks.tracers import LangChainTracer
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks import StdOutCallbackHandler

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# LangSmith ì„¤ì •
handler = StdOutCallbackHandler()
tracer = LangChainTracer(project_name="McDonald")
callback_manager = CallbackManager([handler, tracer])

st.set_page_config(page_title="ëª¨ë‘ì˜ì ì› McDonald version", page_icon="ğŸ§Š")
st.title("ëª¨ë‘ì˜ì ì› McDonald orderbot")
st.markdown(
    """
    ë°˜ê°‘ìŠµë‹ˆë‹¤! ë§¥ë„ë‚ ë“œì—ì„œ í–‰ë³µí•œ ê²½í—˜ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
    """
)

class ChatCallbackHandler(BaseCallbackHandler):
    
    message = ""

    def on_llm_start(self, *args, **kwargs):
        # ë¹ˆ ìœ„ì ¯ì„ ì œê³µí•¨
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
        self.message = ""  # ë©”ì‹œì§€ ì´ˆê¸°í™”
    
    def on_llm_new_token(self, token:str, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

# chatGPTëŠ” streaming ì§€ì›í•¨(ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŒ)
llm = ChatOpenAI(
    temperature=0.1, 
    streaming=True, 
    callbacks=[ChatCallbackHandler()],
    # callback_manager=callback_manager
)


loader = CSVLoader(file_path='files/MD_menu.csv', encoding='UTF-8')
cache_dir = LocalFileStore(f"/home/yoojin/ML/aiffel/HackaThon/nomad_fullstackGPT/.cache/embeddings/McDonald")
splitter = CharacterTextSplitter(separator=",", chunk_size=1000, chunk_overlap=0)

documents = loader.load()
texts =splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
vectorstore = FAISS.from_documents(texts, cached_embeddings)
retriever = vectorstore.as_retriever()
    
    
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì„¤ì •

# Reset ë²„íŠ¼ ì¶”ê°€
if st.button("Reset"):
    st.session_state["messages"] = []
    st.session_state["first_encounter"] = True  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ ì¬ì„¤ì •
    st.cache_resource.clear()
    st.experimental_rerun()

@st.cache_resource
def get_memory():
    return ConversationBufferMemory(
        llm=llm,
        max_token_limit=200,
        return_messages=True,
        memory_key="chat_history"
    )

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """
    You are an automated ordering system working at a McDonald's burger restaurant. 
    Even if the customer's question is not clear, try to make the best guess and provide the closest available option. 
    All responses should be based solely on context and chat history. 
    If you don't know the answer, respond with "I didnâ€™t quite understand." Do not make up answers.

    The process for taking an order is as follows:

    First, greet the customer warmly.
    Then, take the customer's order.
    If the customer has difficulty ordering, recommend new menu items.
    If the customer has specific preferences, recommend the menu item that best matches those preferences.
    Once an order is completed, confirm the order details again.
    When selecting a burger menu, always confirm whether the customer wants a combo meal.
    A combo meal includes a burger, a side, and a drink.
    After the order is finished, review the entire order once more and confirm the payment method.
    
    Context: {context}
    
    Chat History: {chat_history}
    """),
    ("human", "{question}"),
])

message = st.chat_input("Ask a question")
if message:
    st.session_state["messages"].append({"message": message, "role": "human"})
    paint_history()  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¦‰ì‹œ í‘œì‹œ
    
    memory = get_memory()
    chain = {
        "context": retriever | RunnableLambda(format_docs),
        "chat_history": RunnableLambda(lambda _: memory.chat_memory),
        "question": RunnablePassthrough()
    } | prompt | llm
    
    with st.chat_message("ai"):
        response = chain.invoke(message, config={"callbacks": [tracer]})
        ai_response = response.content
        st.markdown(ai_response)  # st.writeë¥¼ st.markdownìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë©”ì‹œì§€ ë°•ìŠ¤ì— ì¶œë ¥
        # save_message(ai_response, "ai")  # AI ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    
    memory.save_context({"input": message}, {"output": ai_response})
    st.experimental_rerun()
else:
    if st.session_state.get("first_encounter", False):
        st.session_state["messages"].append({"message": "ì£¼ë¬¸ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”.", "role": "ai"})
        st.session_state["first_encounter"] = False  # ì²« ë§Œë‚¨ í”Œë˜ê·¸ í•´ì œ
    paint_history()

